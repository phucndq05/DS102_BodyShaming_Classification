# [GHI CH√ö D√ÄNH CHO NH√ìM PH√ÅT TRI·ªÇN]
# -----------------------------------
# T·ªáp tin n√†y cung c·∫•p khung s∆∞·ªùn (template) c∆° b·∫£n cho l·ªõp x·ª≠ l√Ω d·ªØ li·ªáu.
# C√°c th√†nh vi√™n c√≥ quy·ªÅn ch·ªânh s·ª≠a, t·ªëi ∆∞u h√≥a logic b√™n trong c√°c h√†m
# ƒë·ªÉ ph√π h·ª£p v·ªõi y√™u c·∫ßu th·ª±c t·∫ø c·ªßa d·ª± √°n.
# Khuy·∫øn ngh·ªã gi·ªØ nguy√™n t√™n L·ªõp v√† c√°c ph∆∞∆°ng th·ª©c ch√≠nh (process, clean_text)
# ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t∆∞∆°ng th√≠ch khi t√≠ch h·ª£p h·ªá th·ªëng.
# Ch·ª©c nƒÉng: Class x·ª≠ l√Ω d·ªØ li·ªáu chu·∫©n (Stopwords .txt + Teencode .csv)

import pandas as pd
import re
import unicodedata
import emoji  # C·∫ßn pip install emoji
from pyvi import ViTokenizer 
import os
from sklearn.model_selection import train_test_split
from tqdm import tqdm

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, '..', 'data')

class DataPreprocessor:
    def __init__(self, mode='baseline', stopwords_path=None, teencode_path=None):
        """
        Kh·ªüi t·∫°o b·ªô x·ª≠ l√Ω d·ªØ li·ªáu.
        Tham s·ªë:
            stopwords_path: N·∫øu None -> Ch·∫ø ƒë·ªô Deep Learning (Gi·ªØ stopwords).
                        N·∫øu c√≥ path -> Ch·∫ø ƒë·ªô Statistical (X√≥a stopwords).
        """
        self.stopwords = set()
        self.teencode_dict = {} # Kh·ªüi t·∫°o r·ªóng, s·∫Ω load t·ª´ CSV

        # Load stopwords t·ª´ file .txt
        if stopwords_path and os.path.exists(stopwords_path):
            try:
                with open(stopwords_path, 'r', encoding='utf-8') as f:
                    # splitlines() t·ª± ƒë·ªông c·∫Øt d√≤ng, strip() ƒë·ªÉ x√≥a kho·∫£ng tr·∫Øng th·ª´a ƒë·∫ßu ƒëu√¥i
                    self.stopwords = set(line.strip() for line in f if line.strip())
                print(f"[Statistical Mode] ƒê√£ load {len(self.stopwords)} stopwords.")
            except Exception as e:
                print(f" L·ªói load stopwords: {e}")
        else:
            print(f"[Deep Learning Mode] Kh√¥ng d√πng Stopwords (Gi·ªØ nguy√™n vƒÉn b·∫£n).")

        # Load teencode t·ª´ file .csv
        if teencode_path and os.path.exists(teencode_path):
            try:
                df = pd.read_csv(teencode_path)
                # Ki·ªÉm tra xem file c√≥ ƒë√∫ng 2 c·ªôt c·∫ßn thi·∫øt kh√¥ng
                if 'Word' in df.columns and 'Meaning' in df.columns:
                    # Chuy·ªÉn th√†nh Dictionary {Word: Meaning}
                    # √©p ki·ªÉu str ƒë·ªÉ tr√°nh l·ªói n·∫øu file csv c√≥ s·ªë
                    self.teencode_dict = dict(zip(df['Word'].astype(str), df['Meaning'].astype(str)))
                    print(f"ƒê√£ load {len(self.teencode_dict)} teencode t·ª´ file .csv")
                else:
                    print("File CSV thi·∫øu c·ªôt 'Word' ho·∫∑c 'Meaning'")
            except Exception as e:
                print(f"L·ªói load teencode CSV: {e}")
        else:
            print(f"Kh√¥ng t√¨m th·∫•y file teencode t·∫°i: {teencode_path}")

        # Compile Regex
        if self.teencode_dict:
            # S·∫Øp x·∫øp t·ª´ d√†i tr∆∞·ªõc ng·∫Øn sau ƒë·ªÉ replace ƒë√∫ng (vd: 'ko' tr∆∞·ªõc 'k')
            sorted_keys = sorted(self.teencode_dict.keys(), key=len, reverse=True)
            self.teencode_pattern = re.compile(r'\b(' + '|'.join(re.escape(k) for k in sorted_keys) + r')\b')
        else:
            self.teencode_pattern = None

    def clean_text(self, text):
        """
        B∆∞·ªõc 1: Basic Cleaning & Formatting
        M·ª•c ti√™u: L√†m s·∫°ch nhi·ªÖu b·ªÅ m·∫∑t tr∆∞·ªõc khi chu·∫©n h√≥a s√¢u.
        Bao g·ªìm:
        - Chuy·ªÉn to√†n b·ªô vƒÉn b·∫£n v·ªÅ ch·ªØ th∆∞·ªùng (lowercase)
        - Lo·∫°i b·ªè HTML tags
        - Lo·∫°i b·ªè URL / Link
        - Lo·∫°i b·ªè Mentions (@user) theo regex ASCII (tr√°nh ·∫£nh h∆∞·ªüng ti·∫øng Vi·ªát)
        - Lo·∫°i b·ªè Hashtag (#topic)
        - Chu·∫©n h√≥a k√Ω t·ª± xu·ªëng d√≤ng, tab v·ªÅ kho·∫£ng tr·∫Øng
        - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng d∆∞ th·ª´a
        """
        
        if not isinstance(text, str): 
            return ""
        
        # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
        text = text.lower()
        
        # 2. X√≥a HTML tags
        text = re.sub(r'<[^>]*>', ' ', text)
        
        # 3. X√≥a URL/Link
        text = re.sub(r'http\S+|www\.\S+', '', text)
        
        # 4. X√≥a Mentions (@user) ‚Äì d√πng regex ASCII ƒë·ªÉ kh√¥ng d√≠nh ch·ªØ Vi·ªát
        text = re.sub(r'@[a-zA-Z0-9_.]+', '', text)
        
        # 5. X√≥a Hashtag (#trend)
        text = re.sub(r'#\S+', '', text)
        
        # 6. Chu·∫©n h√≥a newline, tab v·ªÅ kho·∫£ng tr·∫Øng
        text = re.sub(r'[\n\t]', ' ', text)
        
        # 7. X√≥a kho·∫£ng tr·∫Øng d∆∞ th·ª´a
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def handle_emoji(self, text):
        """
        B∆Ø·ªöC 4: EMOJI HANDLING
        M·ª•c ti√™u: Chuy·ªÉn emoji sang d·∫°ng text ƒë·ªÉ m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c·∫£m x√∫c.
        C√°ch l√†m:
        - S·ª≠ d·ª•ng th∆∞ vi·ªán emoji.demojize
        - Chuy·ªÉn üò≠ ‚Üí loudly crying face
        - Thay d·∫•u : v√† _ th√†nh kho·∫£ng tr·∫Øng ƒë·ªÉ t√°ch t·ª´"""
        
        # demojize: chuy·ªÉn üò≠ -> :loudly_crying_face:
        text = emoji.demojize(text, delimiters=(' ', ' '))
        
        # loudly_crying_face: -> loudly crying face
        text = text.replace(':', '').replace('_', ' ')
        
        return text

    def normalize(self, text):
        """
        B∆Ø·ªöC 2 ‚Üí 6 : NORMALIZATION PIPELINE
        Bao g·ªìm:
        - B∆∞·ªõc 2: Chu·∫©n h√≥a Unicode (NFC)
        - B∆∞·ªõc 3: Chu·∫©n h√≥a k√Ω t·ª± l·∫∑p (spam characters)
        - B∆∞·ªõc 4: X·ª≠ l√Ω Emoji
        - B∆∞·ªõc 5: Chu·∫©n h√≥a Teencode / Slang
        - B∆∞·ªõc 6: L·ªçc & chu·∫©n h√≥a d·∫•u c√¢u
        """
        # B∆∞·ªõc 2: Chu·∫©n h√≥a Unicode (NFC)
        text = unicodedata.normalize('NFC', text)
        
        # B∆∞·ªõc 4: X·ª≠ l√Ω Emoji 
        text = self.handle_emoji(text)
        
        # X·ª≠ l√≠ ri√™ng cho t·ª´ "kg": 'kilogram' ho·∫∑c l√† 'kh√¥ng'
        # Case A: ƒê∆°n v·ªã ƒëo (5kg ‚Üí 5 kilogram)
        text = re.sub(r'(\d+)\s*kg\b', r'\1 kilogram', text)
        # Case B: Nghƒ©a ph·ªß ƒë·ªãnh (kg ‚Üí kh√¥ng)
        text = re.sub(r'\bkg\b', 'kh√¥ng', text)

        # B∆∞·ªõc 5: Map Teencode (T·ª´ file CSV ƒë√£ load)
        if self.teencode_pattern:
            text = self.teencode_pattern.sub(lambda x: self.teencode_dict[x.group()], text)
        
        # B∆∞·ªõc 3: Spam Character Handling
        # R√∫t g·ªçn k√Ω t·ª± l·∫∑p > 2 l·∫ßn v·ªÅ 1 k√Ω t·ª± g·ªëc (VD: ƒë·∫πpppp -> ƒë·∫πp)
        text = re.sub(r'(.)\1{2,}', r'\1', text)
        
        # B∆∞·ªõc 6: Punctuation Filtering
        # 1. Chu·∫©n h√≥a d·∫•u ba ch·∫•m: ... ho·∫∑c .... -> v·ªÅ chu·∫©n '...'
        text = re.sub(r'\.{3,}', ' ... ', text)
        
        # X√≥a d·∫•u c√¢u nhi·ªÖu:  , - * ~ ( ) 
        # Gi·ªØ l·∫°i: ! ? ...
        text = re.sub(r'[,\-*~()"]', ' ', text)

        # X√≥a d·∫•u ch·∫•m ƒë∆°n (.) nh∆∞ng kh√¥ng ·∫£nh h∆∞·ªüng d·∫•u ba ch·∫•m (...)
        text = re.sub(r'(?<!\.)\.(?!\.)', ' ', text)

        # 2. T√°ch d·∫•u c√¢u (Gi·ªØ l·∫°i ! ? ƒë·ªÉ model h·ªçc c·∫£m x√∫c)
        # VD: "qu√°!" -> "qu√° !"
        text = re.sub(r'([!?]+)', r' \1 ', text)
        
        # X√≥a kho·∫£ng tr·∫Øng th·ª´a sinh ra
        return re.sub(r'\s+', ' ', text).strip()

    def remove_stopwords(self, text):
        """
        B∆Ø·ªöC 8 ‚Äì STOPWORDS REMOVAL
        √Åp d·ª•ng cho m√¥ h√¨nh Statistical (TF-IDF, ML truy·ªÅn th·ªëng).
        Kh√¥ng √°p d·ª•ng cho Deep Learning ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh.
        """
        if not self.stopwords:
            return text
        
        words = text.split()
        # Gi·ªØ l·∫°i t·ª´ kh√¥ng n·∫±m trong stopwords
        words = [w for w in words if w not in self.stopwords]
        return ' '.join(words)


    def process(self, text, target_model='statistical'):
        """
        MAIN PREPROCESSING PIPELINE (NHI·ªÜM V·ª§ 1)
        Lu·ªìng x·ª≠ l√Ω chu·∫©n:
        - B∆∞·ªõc 1: Basic Cleaning
        - B∆∞·ªõc 2‚Äì6: Normalization
        - B∆∞·ªõc 7: Word Segmentation (ViTokenizer)
        - B∆∞·ªõc 8: Stopwords Removal (ch·ªâ cho Statistical Model)
        """
        # B∆∞·ªõc 1
        text = self.clean_text(text) 
        # B∆∞·ªõc 2-6 
        text = self.normalize(text)   
        
        # B∆∞·ªõc 7: T√°ch t·ª´ (b·∫Øt bu·ªôc cho c·∫£ 2 mode)
        text = ViTokenizer.tokenize(text)
        
        # B∆∞·ªõc 8: Ph√¢n nh√°nh x·ª≠ l√Ω Stopwords
        if target_model == 'statistical':
            # Mode Statistical: Ch·∫°y Full 11 b∆∞·ªõc (X√≥a Stopwords)
            text = self.remove_stopwords(text)

        # Mode Deep Learning: Kh√¥ng l√†m g√¨ th√™m (Gi·ªØ nguy√™n text ƒë√£ t√°ch t·ª´)
        # V√¨ PhoBERT c·∫ßn ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß c·ªßa c√¢u.

        return text

   
if __name__ == "__main__":
    # 1. Kh·ªüi t·∫°o
    preprocessor = DataPreprocessor(
        stopwords_path=os.path.join(DATA_DIR, 'dictionaries', 'vietnamese_stopwords.txt'),
        teencode_path=os.path.join(DATA_DIR, 'dictionaries', 'teencode.csv')
    )
    
    # 2. ƒê·ªçc d·ªØ li·ªáu th√¥
    input_file = os.path.join(DATA_DIR, 'raw', 'dataset_raw.csv') 
    if os.path.exists(input_file):
        df = pd.read_csv(input_file)
        df.rename(columns={'comment_text': 'text', 'comment_id': 'id'}, inplace=True)
        
        # 3. Ch·∫°y 2 l·∫ßn Pipeline cho 2 Mode
        modes = ['statistical', 'deep_learning']
        tqdm.pandas(desc="Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")
        for mode in modes:
            print(f"\n ƒêang x·ª≠ l√Ω cho ch·∫ø ƒë·ªô: {mode}")
            temp_df = df.copy()
            
            # Ti·ªÅn x·ª≠ l√Ω text theo mode
            temp_df['text'] = temp_df['text'].progress_apply(lambda x: preprocessor.process(x, target_model=mode))
            
            # Map nh√£n t·ª´ ch·ªØ sang s·ªë
            label_map = {'Kh√¥ng x√∫c ph·∫°m': 0, 'M·ªâa mai': 1, 'X√∫c ph·∫°m': 2}
            temp_df['label'] = temp_df['label'].map(label_map)
        
            # X√≥a c√°c d√≤ng c√≥ nh√£n (label) b·ªã tr·ªëng
            temp_df = temp_df.dropna(subset=['label'])
        
            # X√≥a c√°c d√≤ng c√≥ vƒÉn b·∫£n b·ªã tr·ªëng sau khi x·ª≠ l√Ω (v√≠ d·ª•: comment ch·ªâ c√≥ emoji b·ªã x√≥a h·∫øt)
            temp_df = temp_df[temp_df['text'].str.strip() != '']
        
            # ƒê·∫£m b·∫£o label l√† ki·ªÉu s·ªë nguy√™n (Integer)
            temp_df['label'] = temp_df['label'].astype(int)
        
            # B∆∞·ªõc 9: Deduplication (L·ªçc tr√πng)
            temp_df = temp_df.drop_duplicates(subset=['text'], keep='first')
            
            # B∆∞·ªõc 11: Data Splitting (70-15-15)
            # Split 1: T√°ch Test (15%)
            train_val, test = train_test_split(
                temp_df, test_size=0.15, stratify=temp_df['label'], random_state=42
            )
            # Split 2: T√°ch Train v√† Val (T·ª∑ l·ªá 0.15/0.85 approx 0.1765)
            train, val = train_test_split(
                train_val, test_size=0.1765, stratify=train_val['label'], random_state=42
            )
            
            # X√°c ƒë·ªãnh c√°c c·ªôt c·∫ßn gi·ªØ l·∫°i
            output_cols = ['id', 'text', 'label']

            # Xu·∫•t file (Nhi·ªám v·ª• 2 - ƒê·ªß 6 file)
            suffix = 'stat' if mode == 'statistical' else 'dl'
            processed_dir = os.path.join(DATA_DIR, 'processed')
            os.makedirs(processed_dir, exist_ok=True)
            
            train[output_cols].to_csv(f'{processed_dir}/train_{suffix}.csv', index=False)
            val[output_cols].to_csv(f'{processed_dir}/val_{suffix}.csv', index=False)
            test[output_cols].to_csv(f'{processed_dir}/test_{suffix}.csv', index=False)
            
        print("\nHo√†n th√†nh xu·∫•t 6 file output t·∫°i data/processed/!")
    else:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file ƒë·∫ßu v√†o t·∫°i {input_file}")