"""
Module Ä‘á»‹nh nghÄ©a cÃ¡c lá»›p mÃ´ hÃ¬nh (Model Classes).
Há»— trá»£ cáº£ Deep Learning (PhoBERT) vÃ  Machine Learning cÆ¡ báº£n (SVM, NaiveBayes, LogReg).
"""

import os
import joblib
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Union

# ThÆ° viá»‡n cho Baseline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

# ThÆ° viá»‡n cho PhoBERT
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments
from torch.utils.data import Dataset

# ==========================================
# PHáº¦N 1: CÃC CLASS Há»– TRá»¢ PHOBERT
# ==========================================

class TextDataset(Dataset):
    """Dataset wrapper cho PyTorch/HuggingFace"""
    def __init__(self, encodings, labels=None):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

class WeightedTrainer(Trainer):
    """Trainer tÃ¹y chá»‰nh há»— trá»£ Weighted Cross Entropy Loss"""
    def __init__(self, class_weights, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)
        else:
            self.class_weights = None

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        if self.class_weights is not None:
            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)
            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        else:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
            
        return (loss, outputs) if return_outputs else loss

# ==========================================
# PHáº¦N 2: PHOBERT CLASSIFIER
# ==========================================

class PhoBERTClassifier:
    """Wrapper cho mÃ´ hÃ¬nh PhoBERT"""
    def __init__(self, model_name="vinai/phobert-base", num_labels=3, load_path=None):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # Náº¿u load_path cÃ³ giÃ¡ trá»‹ -> Load model Ä‘Ã£ train Ä‘á»ƒ predict
        path = load_path if load_path else model_name
        
        self.tokenizer = AutoTokenizer.from_pretrained(path)
        self.model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=num_labels).to(self.device)
        self.trainer = None

    def train(self, train_texts, train_labels, val_texts, val_labels, output_dir, class_weights=None, epochs=5):
        # 1. Tokenize
        train_encodings = self.tokenizer(list(train_texts), truncation=True, padding=True, max_length=256)
        val_encodings = self.tokenizer(list(val_texts), truncation=True, padding=True, max_length=256)
        
        # 2. Táº¡o Dataset
        train_ds = TextDataset(train_encodings, train_labels)
        val_ds = TextDataset(val_encodings, val_labels)

        # 3. Cáº¥u hÃ¬nh Training
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            eval_strategy="epoch",
            save_strategy="no",             # KhÃ´ng lÆ°u checkpoint rÃ¡c
            learning_rate=2e-5,
            logging_steps=50,
            report_to="none"                # Táº¯t wandb/mlflow cho gá»n
        )

        # 4. Khá»Ÿi táº¡o Trainer
        self.trainer = WeightedTrainer(
            class_weights=class_weights,
            model=self.model,
            args=training_args,
            train_dataset=train_ds,
            eval_dataset=val_ds,
            compute_metrics=self._compute_metrics
        )
        
        print(f"[PhoBERT] Báº¯t Ä‘áº§u huáº¥n luyá»‡n trÃªn thiáº¿t bá»‹: {self.device}")
        self.trainer.train()

    def evaluate(self, texts, labels):
        # HÃ m thá»‘ng nháº¥t interface vá»›i BaselineModel
        if self.trainer is None: self.trainer = Trainer(model=self.model)
        
        encodings = self.tokenizer(list(texts), truncation=True, padding=True, max_length=256)
        dataset = TextDataset(encodings, labels)
        
        preds = self.trainer.predict(dataset)
        y_pred = np.argmax(preds.predictions, axis=-1)
        
        print("\n--- BÃ¡o cÃ¡o ÄÃ¡nh giÃ¡ (PhoBERT) ---")
        print(classification_report(labels, y_pred))
        return {
            'accuracy': accuracy_score(labels, y_pred),
            'report': classification_report(labels, y_pred, output_dict=True),
            'confusion_matrix': confusion_matrix(labels, y_pred)
        }

    def predict(self, texts):
        # HÃ m dÃ¹ng riÃªng cho dá»± Ä‘oÃ¡n khÃ´ng nhÃ£n
        if self.trainer is None: self.trainer = Trainer(model=self.model)
        encodings = self.tokenizer(list(texts), truncation=True, padding=True, max_length=256)
        dataset = TextDataset(encodings)
        preds = self.trainer.predict(dataset)
        return np.argmax(preds.predictions, axis=-1)

    def save(self, path):
        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)
        print(f"[PhoBERT] ÄÃ£ lÆ°u model táº¡i: {path}")

    @staticmethod
    def _compute_metrics(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        return {'f1_macro': f1_score(labels, preds, average='macro')}

# ==========================================
# PHáº¦N 3: BASELINE MODEL (SVM, NB, LogReg)
# ==========================================

class BaselineModel:
    """
    Class mÃ´ hÃ¬nh há»c mÃ¡y thá»‘ng kÃª cÆ¡ báº£n.
    Sá»­ dá»¥ng Scikit-learn Pipeline (TfidfVectorizer + Classifier).
    """
    
    def __init__(self, model_type: str = 'svm'):
        self.model_type = model_type
        self.pipeline = None
        
    def build_pipeline(self) -> Pipeline:
        # 1. Cáº¥u hÃ¬nh TF-IDF
        tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)
        
        # 2. Cáº¥u hÃ¬nh Classifier
        if self.model_type == 'naive_bayes':
            clf = MultinomialNB()
        elif self.model_type == 'svm':
            clf = SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42)
        elif self.model_type == 'logreg':
            clf = LogisticRegression(solver='lbfgs', class_weight='balanced', max_iter=1000, random_state=42)
        else:
            raise ValueError(f"Model type '{self.model_type}' chÆ°a Ä‘Æ°á»£c há»— trá»£.")
            
        self.pipeline = Pipeline([('tfidf', tfidf), ('clf', clf)])
        return self.pipeline
    
    def train(self, X_train, y_train, **kwargs) -> None: # ThÃªm **kwargs Ä‘á»ƒ há»©ng cÃ¡c tham sá»‘ thá»«a tá»« train.py
        if self.pipeline is None: self.build_pipeline()
        print(f"-> [Baseline] Äang huáº¥n luyá»‡n mÃ´ hÃ¬nh: {self.model_type}...")
        self.pipeline.fit(X_train, y_train)
        print("-> [Baseline] Huáº¥n luyá»‡n hoÃ n táº¥t.")
        
    def evaluate(self, X_test, y_test) -> Dict[str, Any]:
        if self.pipeline is None: raise Exception("MÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n!")
        y_pred = self.pipeline.predict(X_test)
        
        print(f"\n--- BÃ¡o cÃ¡o ÄÃ¡nh giÃ¡ ({self.model_type}) ---")
        print(classification_report(y_test, y_pred))
        return {
            'accuracy': accuracy_score(y_test, y_pred),
            'report': classification_report(y_test, y_pred, output_dict=True),
            'confusion_matrix': confusion_matrix(y_test, y_pred)
        }

    def predict(self, texts):
        return self.pipeline.predict(texts)
    
    def save(self, model_path: str) -> None:
        directory = os.path.dirname(model_path)
        if directory and not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)
        joblib.dump(self.pipeline, model_path)
        print(f"ğŸ’¾ [Baseline] ÄÃ£ lÆ°u model táº¡i: {model_path}")