import streamlit as st
import joblib
import os
import sys
import numpy as np
import torch
import re
import emoji
from pyvi import ViTokenizer
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

st.set_page_config(page_title="Body Shaming Detection", page_icon="üõ°Ô∏è", layout="centered")

# --- 2. H√ÄM X·ª¨ L√ù TEXT (Gi·ªØ nguy√™n logic chu·∫©n) ---
def local_clean_text(text, mode='statistical'):
    if not isinstance(text, str): return ""
    
    text = text.lower()
    text = emoji.demojize(text, delimiters=(' ', ' '))
    text = text.replace(':', '').replace('_', ' ')
    
    text = re.sub(r'<[^>]*>', ' ', text)
    text = re.sub(r'http\S+|www\.\S+', '', text)
    text = re.sub(r'@[a-zA-Z0-9_.]+', '', text)
    text = re.sub(r'#\S+', '', text)
    text = re.sub(r'[\n\t]', ' ', text)
    
    text = re.sub(r'(\d+)\s*kg\b', r'\1 kilogram', text)
    text = re.sub(r'\bkg\b', 'kh√¥ng', text)
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    
    text = re.sub(r'\.{3,}', ' ... ', text)
    text = re.sub(r'[,\-*~()"]', ' ', text)
    text = re.sub(r'(?<!\.)\.(?!\.)', ' ', text)
    text = re.sub(r'([!?]+)', r' \1 ', text)
    
    # T√°ch t·ª´
    text = ViTokenizer.tokenize(text.strip())
    return text

# --- 3. LOAD MODEL ---
ARTIFACTS_DIR = os.path.join(current_dir, "artifacts")
MODEL_CONFIG = {
    "SVM": "svm.pkl",
    "Naive Bayes": "naive_bayes.pkl",
    "Logistic Regression": "logreg.pkl",
    "PhoBERT": "phobert_final"
}

@st.cache_resource
def load_model(model_name):
    model, tokenizer = None, None
    path = os.path.join(ARTIFACTS_DIR, MODEL_CONFIG[model_name])

    if model_name in ["SVM", "Naive Bayes", "Logistic Regression"]:
        if os.path.exists(path):
            try:
                model = joblib.load(path)
            except Exception as e:
                st.error(f"‚ùå L·ªói file {model_name}: {e}")
        else:
            st.error(f"‚ùå Thi·∫øu file: {path}")

    elif model_name == "PhoBERT":
        if os.path.exists(path):
            try:
                tokenizer = AutoTokenizer.from_pretrained(path)
                model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=3)
                model.to("cpu")
                model.eval()
            except Exception as e:
                st.error(f"‚ùå L·ªói load PhoBERT: {e}")
        else:
            st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {path}")
            
    return model, tokenizer

# --- 4. H√ÄM D·ª∞ ƒêO√ÅN ---
def predict(model_obj, tokenizer_obj, text, model_name):
    mode = 'deep_learning' if model_name == 'PhoBERT' else 'statistical'
    clean_txt = local_clean_text(text, mode=mode)
    
    label, confidence = 0, 0.0
    
    if model_name == "PhoBERT":
        if model_obj is None: return 0, 0.0
        inputs = tokenizer_obj(clean_txt, return_tensors="pt", truncation=True, padding=True, max_length=128)
        with torch.no_grad():
            outputs = model_obj(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        probs_np = probs.numpy()[0]
        label = np.argmax(probs_np)
        confidence = probs_np[label]
    else:
        if model_obj is None: return 0, 0.0
        try:
            proba = model_obj.predict_proba([clean_txt])[0]
            label = np.argmax(proba)
            confidence = proba[label]
        except:
            label = model_obj.predict([clean_txt])[0]
            confidence = 1.0
            
    return label, confidence

# --- 5. GIAO DI·ªÜN CH√çNH (Clean Version) ---
def main():
    # Sidebar
    st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh Model")
    model_option = st.sidebar.selectbox("Ch·ªçn Thu·∫≠t to√°n:", list(MODEL_CONFIG.keys()))
    
    with st.spinner(f"ƒêang kh·ªüi ƒë·ªông {model_option}..."):
        model, tokenizer = load_model(model_option)
    if model: st.sidebar.success(f"‚úÖ ƒê√£ load {model_option}")

    # Main UI
    st.title("üõ°Ô∏è Demo Body Shaming Detection")
    st.markdown("---")

    # Ch·ªâ c√≤n √¥ nh·∫≠p li·ªáu ƒë∆°n gi·∫£n
    text_input = st.text_area(
        "üìù Nh·∫≠p b√¨nh lu·∫≠n c·∫ßn ki·ªÉm tra:", 
        height=100, 
        placeholder="V√≠ d·ª•: B·∫°n n√†y nh√¨n ƒë·∫πp qu√°"
    )

    # Button Ph√¢n t√≠ch
    if st.button("üîç Ph√¢n t√≠ch ngay", type="primary"):
        if not text_input.strip():
            st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung!")
        else:
            with st.spinner("AI ƒëang ph√¢n t√≠ch..."):
                pred_label, conf = predict(model, tokenizer, text_input, model_option)
                
                result_map = {
                    0: ("KH√îNG X√öC PH·∫†M", "success", "‚úÖ"),
                    1: ("M·ªàA MAI", "warning", "‚ö†Ô∏è"),
                    2: ("X√öC PH·∫†M", "error", "üö´")
                }
                txt, color, icon = result_map.get(pred_label)
                
                st.markdown(f"### K·∫øt qu·∫£:")
                if color == "success": st.success(f"{icon} {txt}")
                elif color == "warning": st.warning(f"{icon} {txt}")
                else: st.error(f"{icon} {txt}")
                
                st.progress(float(conf), text=f"ƒê·ªô tin c·∫≠y: {conf*100:.2f}%")

if __name__ == "__main__":
    main()